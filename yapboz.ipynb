{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e78c7e8b",
   "metadata": {},
   "source": [
    "## YAPBOZ CHATBOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cc5e4d",
   "metadata": {},
   "source": [
    "### Gerekli Ortamın Kurulumu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2421418e",
   "metadata": {},
   "source": [
    "Gerekli Kütüphaneler:\n",
    "\n",
    "-langchain (dil modelleriyle çalışmayı kolaylaştıran modül)\n",
    "\n",
    "-chromadb (bağlama göre vektörlere çevirdiğimiz metinlerin saklandığı database)\n",
    "\n",
    "-pymupdf (pdf -> yazı)\n",
    "\n",
    "-sentence-transformers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa231082",
   "metadata": {},
   "source": [
    "### 1. Veri Hazırlığı (PDF parçalama ve vektörleştirme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95b3cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz #pymupdf'in import edilirkenki adı (bana da saçma geldi)\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac062ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7781f5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = fitz.open(\"./data/Yapboz.pdf\") # Yapbozu doc değişkenine kaydettik.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "714a1c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_texts = [page.get_text().strip() for page in doc]  # Her sayfayı python listesi\n",
    "                                                     # şeklinde kaydettik.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c2e7d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Şimdi bu sayfaları da daha küçük anlamlı parçalara böleceğiz. (Paragraf->Cümle->Kelime->Harf)\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1000 tokenlik parçalara ayırırken ayırma sırası paragraf cümle diye gidiyor..\n",
    "separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000, # 1000 tokenlik büyüklükte parça\n",
    "    chunk_overlap = 200, # Her bir parça öncekinin son 200 tokeniyle başlayacak (anlam korunur)\n",
    "    length_function = len,\n",
    "    separators=separators\n",
    ")\n",
    "chunks = text_splitter.create_documents(pdf_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d06698",
   "metadata": {},
   "source": [
    "Şimdi, oluşturduğumuz bu parçaları anlam ve bağlamlarına göre sayısal değerlere çevirecek\n",
    "embedding modelini kuralım. Bu da büyük bir dil modeli aslında sadece bu işlem için\n",
    "kodlanmış"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d7be76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yönetici\\AppData\\Local\\Temp\\ipykernel_16336\\3573284172.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  hf_embedding = HuggingFaceEmbeddings(\n",
      "c:\\Users\\yönetici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"intfloat/e5-large-v2\"\n",
    "model_kwargs = {'device':'cpu'}\n",
    "encode_kwargs = {'normalise_embeddings': False}\n",
    "hf_embedding = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "# Şimdilik çalıştırmıyoruz, sadece kurduk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8787306a",
   "metadata": {},
   "source": [
    "Sırada bize cevabı verecek modelimizi kurmakta. Şimdilik ücretsiz bir tercih olan mistral\n",
    "kullanıldı, sistem güzel çalışırsa gpt'ye geçilebilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb932dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yönetici\\AppData\\Local\\Temp\\ipykernel_16336\\1034435967.py:2: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
      "  hf = HuggingFaceHub(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "hf = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    huggingfacehub_api_token=\"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\"\n",
    "    )\n",
    "\n",
    "# Yine aynı şekil, şimdilik kurulumunu yaptık."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1771d356",
   "metadata": {},
   "source": [
    "Sırada ilk başta böldüğümüz parçaların sayısal hallerini saklayacağımız\n",
    "veri tabanını kurmakta. Asıl dil modelimiz buradan aldığı verilerle bize \n",
    "bir cevap üretecek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaa7cd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chroma_client = chromadb.Client(\n",
    "    chromadb.config.Settings(\n",
    "        persist_directory=\"./chroma_db\" # RAM e değil diske yazdırmak için.\n",
    "    )\n",
    ")\n",
    "\n",
    "collection = chroma_client.create_collection(name=\"yapboz\")\n",
    "\n",
    "# Şuanda oluşturduk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b646e025",
   "metadata": {},
   "source": [
    "Bir sonraki adımlarda pdf'i böldüğümüz parçaları hf_embedding ile vektörlere çevirecek,\n",
    "bu vektörleri de oluşturduğumuz veritabanına kaydedeceğiz. Daha sonra bir prompt şablonu\n",
    "oluşturacak, bu şablona ve veritabanındaki vektörlere göre yüklediğimiz dil modeli \n",
    "bizlere bir cevap oluşturacak.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b69938f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yapboz_venv",
   "language": "python",
   "name": "yapboz_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
